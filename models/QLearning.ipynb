{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-16T04:43:44.651814Z",
     "start_time": "2025-03-16T04:43:43.216933Z"
    }
   },
   "source": [
    "from pyqlearning.qlearning.greedy_q_learning import GreedyQLearning\n",
    "from numpy.random import normal"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T04:43:44.667541Z",
     "start_time": "2025-03-16T04:43:44.651814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ],
   "id": "e3172e7472865995",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:32:49.673506Z",
     "start_time": "2025-03-15T08:32:49.647082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PriceReinforcementLearning(GreedyQLearning):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.set_alpha_value(1.0)\n",
    "        self.set_epsilon_greedy_rate(0.8)\n",
    "\n",
    "    def extract_possible_actions(self, state_key):\n",
    "        return list(filter(lambda x: 1 <= x <= 5, (state_key - 1, state_key, state_key + 1)))\n",
    "\n",
    "    def observe_reward_value(self, state_key, action_key):\n",
    "        return 5 - action_key\n",
    "\n",
    "    def learn(self, a, limit=1000):\n",
    "        super().learn(a, limit)\n",
    "        print(self.q_df.sort_values(\"q_value\", ascending=False))"
   ],
   "id": "509f13a4cd33d736",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:32:49.943403Z",
     "start_time": "2025-03-15T08:32:49.913673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ql = PriceReinforcementLearning()\n",
    "for i in range(1, 6):\n",
    "    for j in range(max(1, i - 1), min(6, i + 2)):\n",
    "        ql.save_q_df(i, j, 1.0)"
   ],
   "id": "b5bbc45430eca322",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:32:54.972072Z",
     "start_time": "2025-03-15T08:32:50.785080Z"
    }
   },
   "cell_type": "code",
   "source": "ql.learn(1, limit=1000)",
   "id": "1234189c0f11d7f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   state_key  action_key   q_value\n",
      "0        1.0         1.0  8.000000\n",
      "0        2.0         1.0  8.000000\n",
      "0        1.0         2.0  7.000000\n",
      "0        3.0         2.0  7.000000\n",
      "0        2.0         2.0  7.000000\n",
      "0        4.0         3.0  5.500000\n",
      "0        2.0         3.0  5.500000\n",
      "0        3.0         4.0  3.750000\n",
      "0        4.0         4.0  2.000000\n",
      "0        5.0         4.0  2.000000\n",
      "0        3.0         3.0  1.000000\n",
      "0        4.0         5.0  0.999992\n",
      "0        5.0         5.0  0.500000\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T05:04:26.117086Z",
     "start_time": "2025-03-16T05:04:26.082438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InventoryReinforcementLearning(GreedyQLearning):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.set_alpha_value(1.0)\n",
    "\n",
    "    def learn(self, state_key, limit=1000):\n",
    "        '''\n",
    "        Learning and searching the optimal solution.\n",
    "        \n",
    "        Args:\n",
    "            state_key:      Initial state.\n",
    "            limit:          The maximum number of iterative updates based on value iteration algorithms.\n",
    "        '''\n",
    "        self.t = 1\n",
    "        while self.t <= limit:\n",
    "            next_action_list = self.extract_possible_actions(state_key)\n",
    "            if len(next_action_list):\n",
    "                action_key = self.select_action(\n",
    "                    state_key=state_key,\n",
    "                    next_action_list=next_action_list\n",
    "                )\n",
    "                reward_value, next_state_key = self.observe_reward_value(state_key, action_key)\n",
    "\n",
    "            if len(next_action_list):\n",
    "                # Max-Q-Value in next action time.\n",
    "\n",
    "                next_next_action_list = self.extract_possible_actions(next_state_key)\n",
    "                next_action_key = self.predict_next_action(next_state_key, next_next_action_list)\n",
    "                next_max_q = self.extract_q_df(next_state_key, next_action_key)\n",
    "\n",
    "                # Update Q-Value.\n",
    "                self.update_q(\n",
    "                    state_key=state_key,\n",
    "                    action_key=action_key,\n",
    "                    reward_value=reward_value,\n",
    "                    next_max_q=next_max_q\n",
    "                )\n",
    "                # Update State.\n",
    "                state_key = next_state_key\n",
    "\n",
    "            # Normalize.\n",
    "            self.normalize_q_value()\n",
    "            self.normalize_r_value()\n",
    "\n",
    "            # Vis.\n",
    "            self.visualize_learning_result(state_key)\n",
    "            # Check.\n",
    "            if self.check_the_end_flag(state_key) is True:\n",
    "                break\n",
    "\n",
    "            # Epsode.\n",
    "            self.t += 1\n",
    "\n",
    "        print(self.q_df[self.q_df.state_key <= 30].sort_values([\"state_key\", \"action_key\"]))\n",
    "\n",
    "    def extract_possible_actions(self, state_key):\n",
    "        return [0, 1]\n",
    "\n",
    "    def observe_reward_value(self, state_key, action_key):\n",
    "        if action_key == 1:\n",
    "            state_key += 10\n",
    "        purchased = int(normal(5, 1))\n",
    "        state_key -= purchased\n",
    "        reward = 0\n",
    "        if action_key == 1:\n",
    "            if state_key > 0:\n",
    "                reward = state_key * -0.1\n",
    "        else:\n",
    "            if state_key < 0:\n",
    "                reward = -5\n",
    "        return  reward, state_key if state_key > 0 else 10"
   ],
   "id": "d081db3ad3cd6fd7",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T05:04:30.917787Z",
     "start_time": "2025-03-16T05:04:26.732978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ql = InventoryReinforcementLearning()\n",
    "ql.learn(10)"
   ],
   "id": "21229cf475e012f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   state_key  action_key   q_value\n",
      "0          1         0.0 -5.329688\n",
      "0          1         1.0 -1.150400\n",
      "0          2         0.0 -5.183588\n",
      "0          2         1.0 -0.699319\n",
      "0          3         0.0 -5.119149\n",
      "0          3         1.0 -1.112402\n",
      "0          4         0.0 -0.195129\n",
      "0          4         1.0 -1.450672\n",
      "0          5         0.0 -0.087415\n",
      "0          5         1.0 -0.914321\n",
      "0          6         0.0 -0.575200\n",
      "0          6         1.0 -1.525800\n",
      "0          7         0.0 -2.931616\n",
      "0          7         1.0 -1.412415\n",
      "0          8         0.0 -0.931518\n",
      "0          8         1.0 -1.414290\n",
      "0          9         0.0 -2.957908\n",
      "0          9         1.0 -1.601600\n",
      "0         10         0.0 -0.811765\n",
      "0         10         1.0 -2.010746\n",
      "0         11         0.0 -0.064131\n",
      "0         11         1.0 -1.879381\n",
      "0         12         0.0 -0.851318\n",
      "0         12         1.0 -2.104130\n",
      "0         13         0.0 -0.203200\n",
      "0         13         1.0 -1.962244\n",
      "0         14         0.0 -0.789690\n",
      "0         14         1.0 -1.956387\n",
      "0         15         0.0 -0.331098\n",
      "0         15         1.0 -2.207416\n",
      "0         16         0.0 -0.206415\n",
      "0         16         1.0 -2.048913\n",
      "0         17         0.0 -0.024391\n",
      "0         17         1.0 -2.324188\n",
      "0         18         0.0 -0.451204\n",
      "0         18         1.0 -2.317166\n",
      "0         19         0.0 -0.449803\n",
      "0         19         1.0 -3.398242\n",
      "0         20         0.0 -0.223531\n",
      "0         20         1.0 -3.920313\n",
      "0         21         0.0 -0.426073\n",
      "0         21         1.0 -3.250000\n",
      "0         22         0.0 -0.214831\n",
      "0         22         1.0 -4.162500\n",
      "0         23         0.0 -0.240759\n",
      "0         23         1.0 -3.318750\n",
      "0         24         0.0 -0.099651\n",
      "0         24         1.0 -2.900000\n",
      "0         25         0.0 -0.859375\n",
      "0         25         1.0 -3.970313\n",
      "0         26         0.0 -0.121825\n",
      "0         26         1.0 -3.100000\n",
      "0         27         0.0 -0.047991\n",
      "0         27         1.0 -4.421716\n",
      "0         28         0.0 -0.579297\n",
      "0         29         0.0  0.000000\n",
      "0         29         1.0 -3.662500\n",
      "0         30         1.0 -3.662500\n"
     ]
    }
   ],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
